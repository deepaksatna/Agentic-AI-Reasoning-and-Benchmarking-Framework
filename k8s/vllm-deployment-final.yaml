# vLLM Deployment - Using Mixtral-8x7B (supported by vLLM 0.6.6)
apiVersion: v1
kind: Pod
metadata:
  name: vllm-arbm
  namespace: arbm-benchmark
  labels:
    app: vllm
spec:
  hostNetwork: true
  containers:
  - name: vllm
    image: docker.io/vllm/vllm-openai:v0.6.6
    imagePullPolicy: IfNotPresent
    ports:
    - containerPort: 8000
      hostPort: 8000
    command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    args:
    - "--model"
    - "mistralai/Mixtral-8x7B-Instruct-v0.1"
    - "--tensor-parallel-size"
    - "4"
    - "--max-model-len"
    - "4096"
    - "--gpu-memory-utilization"
    - "0.85"
    - "--trust-remote-code"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8000"
    env:
    - name: HF_HOME
      value: "/mnt/fss/models/hub"
    - name: HF_HUB_CACHE
      value: "/mnt/fss/models/hub"
    - name: HUGGING_FACE_HUB_TOKEN
      value: "<YOUR_HF_TOKEN>"  # Replace with your Hugging Face token
    - name: CUDA_VISIBLE_DEVICES
      value: "0,1,2,3"
    - name: NCCL_DEBUG
      value: "INFO"
    resources:
      limits:
        nvidia.com/gpu: 4
        memory: "128Gi"
        cpu: "32"
      requests:
        nvidia.com/gpu: 4
        memory: "96Gi"
        cpu: "16"
    volumeMounts:
    - name: fss
      mountPath: /mnt/fss
    - name: shm
      mountPath: /dev/shm
  volumes:
  - name: fss
    hostPath:
      path: /mnt/coecommonfss/llmcore
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: 16Gi
  tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
  restartPolicy: Never
