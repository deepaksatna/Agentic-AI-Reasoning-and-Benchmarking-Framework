apiVersion: batch/v1
kind: Job
metadata:
  name: arbm-llama-benchmark
  namespace: arbm-benchmark
spec:
  backoffLimit: 1
  ttlSecondsAfterFinished: 86400
  template:
    spec:
      restartPolicy: Never
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: benchmark
        image: docker.io/vllm/vllm-openai:v0.6.6
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 4
            memory: "128Gi"
            cpu: "32"
          requests:
            nvidia.com/gpu: 4
            memory: "64Gi"
            cpu: "16"
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        - name: MODEL_PATH
          value: "/mnt/fss/2026-NIM-vLLM_LLM/models/llama-3-8b-instruct"
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "=== ARBM Benchmark - Llama-3-8B-Instruct ===" && nvidia-smi && echo "" && python3 -m vllm.entrypoints.openai.api_server --model /mnt/fss/2026-NIM-vLLM_LLM/models/llama-3-8b-instruct --tensor-parallel-size 4 --max-model-len 4096 --gpu-memory-utilization 0.9 --dtype auto --host 0.0.0.0 --port 8000 &
          VLLM_PID=$!
          for i in $(seq 1 60); do curl -s http://localhost:8000/health > /dev/null 2>&1 && break || sleep 5; done
          sleep 10
          python3 /mnt/fss/ARBM/scripts/run_vllm_benchmark.py
          kill $VLLM_PID 2>/dev/null || true
        volumeMounts:
        - name: fss
          mountPath: /mnt/fss
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: fss
        hostPath:
          path: /mnt/coecommonfss/llmcore
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 16Gi
