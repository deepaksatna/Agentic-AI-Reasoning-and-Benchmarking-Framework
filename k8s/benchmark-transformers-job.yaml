apiVersion: batch/v1
kind: Job
metadata:
  name: arbm-benchmark
  namespace: arbm-benchmark
spec:
  backoffLimit: 1
  ttlSecondsAfterFinished: 86400
  template:
    spec:
      restartPolicy: Never
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      containers:
      - name: benchmark
        image: docker.io/vllm/vllm-openai:v0.6.6
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 4
            memory: "128Gi"
            cpu: "32"
          requests:
            nvidia.com/gpu: 4
            memory: "96Gi"
            cpu: "16"
        env:
        - name: HF_HOME
          value: "/mnt/fss/models/hub"
        - name: HF_HUB_CACHE
          value: "/mnt/fss/models/hub"
        - name: HUGGING_FACE_HUB_TOKEN
          value: "<YOUR_HF_TOKEN>"  # Replace with your Hugging Face token
        - name: CUDA_VISIBLE_DEVICES
          value: "0,1,2,3"
        - name: PYTHONUNBUFFERED
          value: "1"
        command: ["/bin/bash", "-c"]
        args:
        - |
          echo "=== GPU Check ==="
          nvidia-smi
          echo ""
          echo "=== Running Benchmark ==="
          python3 /mnt/fss/ARBM/scripts/run_transformers_benchmark.py
        volumeMounts:
        - name: fss
          mountPath: /mnt/fss
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: fss
        hostPath:
          path: /mnt/coecommonfss/llmcore
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 16Gi
